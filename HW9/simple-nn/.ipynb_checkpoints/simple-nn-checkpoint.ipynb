{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Neural net experiments</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>1. Various helper functions</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function loads in a data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datafile):\n",
    "    data = np.loadtxt(datafile)\n",
    "    n,p = data.shape\n",
    "    rawx = data[:,0:2]\n",
    "    rawy = data[:,2]\n",
    "    x = torch.tensor(rawx, dtype=torch.float)\n",
    "    y = torch.reshape(torch.tensor((rawy+1.0)/2.0, dtype=torch.float), [n,1])\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function plots the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(x,y):\n",
    "    x_min = min(x[:,0]) - 1\n",
    "    x_max = max(x[:,0]) + 1\n",
    "    y_min = min(x[:,1]) - 1\n",
    "    y_max = max(x[:,1]) + 1\n",
    "    pos = (torch.squeeze(y) == 1)\n",
    "    neg = (torch.squeeze(y) == 0)\n",
    "    plt.plot(x[pos,0], x[pos,1], 'ro')\n",
    "    plt.plot(x[neg,0], x[neg,1], 'k^')\n",
    "    plt.xlim(x_min,x_max)\n",
    "    plt.ylim(y_min,y_max)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function plots a decision boundary as well as the data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boundary(x,y,model):\n",
    "    \n",
    "    x_min = min(x[:,0]) - 1\n",
    "    x_max = max(x[:,0]) + 1\n",
    "    y_min = min(x[:,1]) - 1\n",
    "    y_max = max(x[:,1]) + 1\n",
    "\n",
    "    delta = 0.05\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, delta), np.arange(y_min, y_max, delta))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    gn, gp = grid.shape\n",
    "    Z = np.zeros(gn)\n",
    "    for i in range(gn):\n",
    "        pred = model(torch.tensor(grid[i,:], dtype=torch.float))\n",
    "        Z[i] = int(pred > 0.5)\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=plt.cm.PRGn, vmin=-3, vmax=3)\n",
    "\n",
    "    # Plot also the training points\n",
    "    pos = (torch.squeeze(y) == 1)\n",
    "    neg = (torch.squeeze(y) == 0)\n",
    "    plt.plot(x[pos,0], x[pos,1], 'ro')\n",
    "    plt.plot(x[neg,0], x[neg,1], 'k^')\n",
    "\n",
    "    plt.xlim(x_min,x_max)\n",
    "    plt.ylim(y_min,y_max)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function computes the error rate of the predicted labels `y1` given the true labels `y2`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate(y1, y2):\n",
    "    sum = 0.0\n",
    "    for i in range(0,y1.size()[0]):\n",
    "        sum += ((y1[i]-0.5) * (y2[i]-0.5) <= 0.0)\n",
    "    return int(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>2. Experiments with toy data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in one of the data sets and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = load_data('data1.txt')\n",
    "plot_data(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train a feedforward net on it. This takes many iterations of gradient descent (backpropagation). We'll print the status every 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train a neural net\n",
    "#\n",
    "# d is input dimension\n",
    "# H is hidden dimension\n",
    "d = 2\n",
    "H = 4\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(d, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, 1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Binary Cross Entropy (BCE) as our loss function.\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "prev_loss = 1.0\n",
    "learning_rate = 0.25\n",
    "done = False\n",
    "t = 1\n",
    "tol = 1e-4\n",
    "while not(done):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(x)\n",
    "    t = t+1\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 1000 == 0:\n",
    "        print('Iteration %d: loss %0.5f errors %d' % \n",
    "              (t, loss.item(), error_rate(y_pred, y)))\n",
    "        if (prev_loss - loss.item() < tol):\n",
    "            done = True\n",
    "        prev_loss = loss.item()\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * (1.0/np.sqrt(t)) * param.grad\n",
    "print(\"Number of training errors:\", error_rate(model(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what kind of a boundary we got!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boundary(x,y,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>3. A different data set</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the next cell generates a data set of 800 points in which the labels are noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 800\n",
    "np.random.seed(0)\n",
    "X_train = np.random.rand(n,2)\n",
    "x1 = X_train[:,0]\n",
    "x2 = X_train[:,1]\n",
    "y_train = ((np.exp(-((x1-0.5)*6)**2)*2*((x1-0.5)*6)+1)/2-x2)>0 \n",
    "\n",
    "idx = np.random.choice(range(n),size=(int(n*0.03),))\n",
    "y_train[idx] = ~y_train[idx]\n",
    "x = torch.tensor(X_train, dtype=torch.float) * 10\n",
    "y = torch.reshape(torch.tensor(y_train, dtype=torch.float), [n,1])\n",
    "plot_data(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='magenta'>Define a neural net with <b>two</b> hidden layers, each containing the same number of nodes. <em>Hint:</em> Start with the code above and just make a small tweak to it.</font>\n",
    "\n",
    "<font color='magenta'>Train the net a few times, and print the decision boundary for the best (lowest-error) model that you find.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
